{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKfPLIJgXdyBTU+1DX69lF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Daudi232/DS_1/blob/main/NLP_Hugging_Face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WEEK 6:1"
      ],
      "metadata": {
        "id": "avrTuXW3pUr4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_e-Y2fygpOGw"
      },
      "outputs": [],
      "source": [
        "## QUIZ\n",
        "# demonstration of grouped entities\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "ner = pipeline(\"ner\", grouped_entities=True)\n",
        "\n",
        "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ner = pipeline (\"ner\", grouped_entities = True)\n",
        "\n",
        "# grouped entities groups the entities mentioned in the text.. eg Anna Sandor as a person\n",
        "\n",
        "ner (\"Her name is Anna Sandor. She is a masters student in Sports Economics. She is cute, but currently sick, Hope she gets better soon!\")"
      ],
      "metadata": {
        "id": "VgAd2iamp1Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#<mask> tag is different depending with model you are using\n",
        "fill_blank = pipeline (\"fill-mask\", model = 'distilroberta-base')\n",
        "\n",
        "fill_blank (\"Her name is Anna, she is so <mask>\")"
      ],
      "metadata": {
        "id": "nfa-1LNJrZtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you should specify candidate labels as an input\n",
        "# the model did not have the labels specified here during its training\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "result = classifier(\"This is a course about the Transformers in NLP\", candidate_labels = ['education', 'sports', 'movies'])\n",
        "print (result)"
      ],
      "metadata": {
        "id": "_6KTNdq5qfJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WEEK 6: 2"
      ],
      "metadata": {
        "id": "DFWyCVn1ifdx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cgn2wN6bS7cK"
      },
      "outputs": [],
      "source": [
        "#!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentenses to use in classification\n",
        "\n",
        "s1 = 'My name is Daudi. I kind of like Manchester united.. only when they win'\n",
        "s2 = 'My relationship with Irene is a rollercoaster, sometimes love, sometimes hate, sometimes joy, sometimes anger'\n",
        "s3 = 'My students are funny, one minute they are engaged, another they are bored. I have a good time with them though'"
      ],
      "metadata": {
        "id": "-bGKFGXLfLwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJ-7D5XhS7cL"
      },
      "outputs": [],
      "source": [
        "# SA using pipeline\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "classifier(\n",
        "    [s1, s2, s3]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to re-do the above following the process.. a little bit low level\n",
        "\n",
        "## TOKENIZATION\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english' # get the model name\n",
        "tokenizer = AutoTokenizer.from_pretrained (checkpoint) # get the tokenizer\n",
        "\n",
        "raw = [s1, s2, s3]\n",
        "\n",
        "# pass in your sentenses for tokenization and conversion to input ids\n",
        "# I have not yet looked at what padding and truncation do\n",
        "# I did not experiment on the effect it has if you leave them as false\n",
        "\n",
        "output_tokenizer = tokenizer (raw, padding = True, truncation = True, return_tensors='tf')\n",
        "print (output_tokenizer)"
      ],
      "metadata": {
        "id": "vD-4Ve9Wf179"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (output_tokenizer['attention_mask'])\n",
        "print(output_tokenizer['input_ids'])\n"
      ],
      "metadata": {
        "id": "KLdxSOZLZd6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## EMBEDDING PART using TFAutoModel\n",
        "\n",
        "from transformers import TFAutoModel\n",
        "\n",
        "model = TFAutoModel.from_pretrained (checkpoint)\n",
        "vec_embs = model (output_tokenizer) #vector embeddings\n",
        "print (vec_embs)"
      ],
      "metadata": {
        "id": "U8fwLOz3fPs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# different ways of getting the vector embeddings\n",
        "\n",
        "vec_embs ['last_hidden_state']\n",
        "\n",
        "vec_embs [0]\n",
        "\n",
        "vec_embs.last_hidden_state"
      ],
      "metadata": {
        "id": "R_bGTI8lhgSK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Using the model-head specific for a certain task and bypassing the embedding step\n",
        "\n",
        "from transformers import TFAutoModelForSequenceClassification #a mouthful\n",
        "\n",
        "model_head = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "model_head_output = model_head (output_tokenizer)\n",
        "\n",
        "print (model_head_output)"
      ],
      "metadata": {
        "id": "COd96HIxiaoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type (model_head_output) #TFSequenceClassifier"
      ],
      "metadata": {
        "id": "pruqhboIjz1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# these are like the log odds-ratio between the 2 classes\n",
        "# looks like the output layer of the model-head is just linear\n",
        "# remember the logistic assumption: that logit = hypothesis (dot-product of input and weights)\n",
        "\n",
        "model_head_output['logits']\n",
        "\n",
        "model_head_output.logits\n",
        "#weird why this does not work.. thought it may considering it is part of the output\n",
        "#model_head_output.dtype"
      ],
      "metadata": {
        "id": "Y7T-OY7ykFfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "NFKVg_qUk4sK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-Processing\n",
        "# converting the logits to probabilities\n",
        "\n",
        "model_sentiment_scores = tf.math.softmax (model_head_output.logits)\n",
        "model_sentiment_scores"
      ],
      "metadata": {
        "id": "m9TFrc-OkWe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the configuration of the model\n",
        "# shows the configuration of the model\n",
        "\n",
        "model.config"
      ],
      "metadata": {
        "id": "28ha4KV4nv61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking out labels\n",
        "\n",
        "model.config.id2label"
      ],
      "metadata": {
        "id": "qxx7D437oUqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# negative sentiment scores for the sentences\n",
        "\n",
        "# first word was considered negative\n",
        "model_sentiment_scores[:,0]"
      ],
      "metadata": {
        "id": "Gi8cBwJln_Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# positive sentiment scores for the sentences\n",
        "model_sentiment_scores[:,1]"
      ],
      "metadata": {
        "id": "W7r1T-x1olNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig, TFBertModel\n",
        "\n",
        "# Taking a look at the config that was used by bert during its training\n",
        "config_bert = BertConfig()\n",
        "\n",
        "# Building a model from scratch using the bert-configuration\n",
        "model_bert = TFBertModel(config_bert)"
      ],
      "metadata": {
        "id": "RKP6gJ3_vOvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## INSIDE THE MODEL\n",
        "\n",
        "from transformers import GPT2Config, TFGPT2Model\n",
        "\n",
        "# Building the configuration\n",
        "\n",
        "config_gpt = GPT2Config()\n",
        "\n",
        "# Building a model from config\n",
        "model_gpt = TFGPT2Model (config_gpt)\n",
        "\n",
        "print (config_gpt)\n",
        "# even the names of the configuration parameters is not identical\n",
        "print (config_bert)"
      ],
      "metadata": {
        "id": "Vb3C9usZu54G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I have got no way of measuring the embedding vectors that have been generated here but they should be shit since the weights\n",
        "# used here are just randomly initialized\n",
        "# these weights are to be used if we want to pre-train this model from scratch..\n",
        "\n",
        "model_gpt(output_tokenizer)"
      ],
      "metadata": {
        "id": "cDtqBVXKwjvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instead of using the random weights, we can use the pre-trained weights\n",
        "\n",
        "model_gpt_pt = TFGPT2Model.from_pretrained ('gpt2')\n",
        "model_gpt_pt(output_tokenizer) # gives us the embeddings of the input ids that were generated by AutoTokenizer\n",
        "\n",
        "# Alternative to the TFGPT2Model.. this is more general\n",
        "# remember this is just for the embeddings, not a specific task\n",
        "model_gpt_auto = TFAutoModel.from_pretrained ('gpt2')"
      ],
      "metadata": {
        "id": "SShoJcG6wnAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the model parameters\n",
        "\n",
        "model_gpt_pt.save_pretrained(\"GPT2_model\")"
      ],
      "metadata": {
        "id": "ieIQBTSryzcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Re-doing the above.. this time using simple words\n",
        "\n",
        "import numpy as np\n",
        "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]\n",
        "\n",
        "encoded_sequences = np.array ([\n",
        "    [101, 7592, 999, 102],\n",
        "    [101, 4658, 1012, 102],\n",
        "    [101, 3835, 999, 102],\n",
        "])\n",
        "\n",
        "encoded_sequences_tensor = tf.constant (encoded_sequences)"
      ],
      "metadata": {
        "id": "y60XMTNOz7sr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using a list as an input for the model does not work\n",
        "# using a numpy array or tensor does work\n",
        "model_gpt_pt (encoded_sequences)"
      ],
      "metadata": {
        "id": "iUAssgXf08j0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WEEK 7"
      ],
      "metadata": {
        "id": "NOcMDYFxC39e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We want to re-do the above following the process.. a little bit low level\n",
        "\n",
        "## TOKENIZATION\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = 'distilbert-base-uncased-finetuned-sst-2-english' # get the model name\n",
        "tokenizer = AutoTokenizer.from_pretrained (checkpoint) # get the tokenizer\n",
        "\n",
        "raw = [s1, s2, s3]\n",
        "\n",
        "# pass in your sentenses for tokenization and conversion to input ids\n",
        "# I have not yet looked at what padding and truncation do\n",
        "# I did not experiment on the effect it has if you leave them as false\n",
        "\n",
        "output_tokenizer = tokenizer (raw, padding = True, truncation = True, return_tensors='tf')\n",
        "print (output_tokenizer)"
      ],
      "metadata": {
        "id": "2ZEqHrTrQ8uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer #i use transformer a lot instead of transformers\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained ('distilbert-base-uncased-finetuned-sst-2-english')\n",
        "\n",
        "# Sentenses to use in tokenization\n",
        "s1 = 'My name is Daudi. I kind of like Manchester united.. only when they win!'\n",
        "s2 = 'My relationship with Irene is a rollercoaster, sometimes love, sometimes hate, sometimes joy, sometimes anger'\n",
        "s3 = 'My students are funny, one minute they are engaged, another they are bored. I have a good time with them though!'\n",
        "\n",
        "# looks like tokenizing many sequences at the same time is not applicable here\n",
        "\n",
        "raw = (s1,s2,s3)\n",
        "tokenized_sentense = tokenizer.tokenize (s1)\n",
        "\n",
        "print (tokenized_sentense)"
      ],
      "metadata": {
        "id": "4jVt7TJRQtOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization using different models\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained ('distilbert-base-uncased-finetuned-sst-2-english')\n",
        "tokenizer2 = AutoTokenizer.from_pretrained (\"bert-base-cased\") # same tokenization as distilbert.. ## used\n",
        "tokenizer3 = AutoTokenizer.from_pretrained (\"gpt2\") ##G used in middle words\n",
        "\n",
        "tokenizer2.tokenize(s1)\n",
        "tokenizer3.tokenize(s1)"
      ],
      "metadata": {
        "id": "-V9-hUesRc4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(s1)"
      ],
      "metadata": {
        "id": "adXkPeBSdJzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer([s1,s2])"
      ],
      "metadata": {
        "id": "hMPRHK-ndqT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding tokens\n",
        "\n",
        "encoded_sequence = tokenizer3.convert_tokens_to_ids(tokenizer3.tokenize(s1))\n",
        "print (encoded_sequence)"
      ],
      "metadata": {
        "id": "5YwCxDcLSTDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoding tokens\n",
        "\n",
        "decoded_sequence = tokenizer3.decode (encoded_sequence)\n",
        "print (decoded_sequence)"
      ],
      "metadata": {
        "id": "Hfs9p8NYXvFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "enc_seq_tensor = tf.constant (encoded_sequence)"
      ],
      "metadata": {
        "id": "hSBL2XYyYCEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc_seq_tensor"
      ],
      "metadata": {
        "id": "lFPWM3BtY_iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel, TFAutoModelForSequenceClassification\n",
        "\n",
        "model = TFAutoModel.from_pretrained('gpt2') #the number of times i forget .from_pretrained is insane\n",
        "model_head = TFAutoModelForSequenceClassification.from_pretrained ('gpt2')\n",
        "\n",
        "emb_vec = model(enc_seq_tensor) #embeddings are generated\n",
        "model_head_output = model_head(enc_seq_tensor) #\n",
        "\n",
        "print (model_head_output) # i expected logits as to whether the sequence is +ve or -ve but got embeddings instead"
      ],
      "metadata": {
        "id": "EPiyKwrxZF0F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#gpt_tokenizer = tokenizer3(s1, padding = True, truncation = True, return_tensors = 'tf')\n",
        "gpt_tokenizer = tokenizer3(s1, truncation = True, return_tensors = 'tf')\n",
        "model_head_output_2 = model_head(gpt_tokenizer)"
      ],
      "metadata": {
        "id": "BqGFCWKLiQGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_head_output_2"
      ],
      "metadata": {
        "id": "D6jwtCyYipRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = tf.constant(ids) # it was a 1 rank-tensor\n",
        "# This line will fail. #actually it has worked pretty well\n",
        "print(model(input_ids))"
      ],
      "metadata": {
        "id": "4r1wh-2Has_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the point of the next cell, but it is not necessary\n",
        "print (tf.constant([1,2,3]))\n",
        "print (tf.constant([[1,2,3]]))"
      ],
      "metadata": {
        "id": "_Yk-S5_dsnMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = tf.constant([ids]) #2 ranked tensor\n",
        "# This line will fail. #actually it has worked pretty well\n",
        "print(model(input_ids))"
      ],
      "metadata": {
        "id": "clljlQ99gbzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Using gpt2 for classification like this won't work\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "#checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained('gpt2')\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_ids = tf.constant(ids)\n",
        "print(model(input_ids)) # using gpt2 will give out embeddings instead of logits"
      ],
      "metadata": {
        "id": "fmhOjK8wlIRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## This should have been how gpt2 could be used, but the results are gibberish\n",
        "\n",
        "# from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "# model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# inputs = tokenizer(sequence, return_tensors='tf')\n",
        "# outputs = model.generate(**inputs, max_length=50)\n",
        "# print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "77iKHXiUrmjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Batching sequences of different lengths\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "sequence1_ids = [[200, 200, 200]]\n",
        "sequence2_ids = [[200, 200]]\n",
        "\n",
        "pad_token_id = 100\n",
        "\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, pad_token_id]\n",
        "]\n",
        "\n",
        "print (model(tf.constant(sequence1_ids)))\n",
        "print (model(tf.constant(sequence2_ids)))\n",
        "\n",
        "print (model(tf.constant(batched_ids)))"
      ],
      "metadata": {
        "id": "Z2gPSqZZr7Tv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Batching sequences of different lengths\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "# tokenizer.pad_token_id\n",
        "\n",
        "sequence1_ids = [[200, 200, 200]]\n",
        "sequence2_ids = [[200, 200]]\n",
        "\n",
        "pad_token_id = 100\n",
        "# tokenizer.pad_token_id\n",
        "# the above is the pad_token_id that is used.. it is the number 0 actually!\n",
        "\n",
        "batched_ids = [\n",
        "    [200, 200, 200],\n",
        "    [200, 200, pad_token_id]\n",
        "]\n",
        "\n",
        "# attention mask helps in ignoring the padded ids that are helpful in matching lengths of sequences\n",
        "attention_mask = [[1, 1, 1],\n",
        "                  [1, 1, 0]\n",
        "                  ]\n",
        "\n",
        "print (model(tf.constant(sequence1_ids)))\n",
        "print (model(tf.constant(sequence2_ids)))\n",
        "\n",
        "# attention mask enabled\n",
        "print (model(tf.constant(batched_ids), attention_mask = tf.constant(attention_mask)))\n"
      ],
      "metadata": {
        "id": "deCG1BHOsc1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## PUTTING IT ALL TOGETHER\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "model_inputs = tokenizer(sequence)\n",
        "print (model_inputs)"
      ],
      "metadata": {
        "id": "lpM3NXPvtJwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "model_inputs = tokenizer(sequences)\n",
        "print (model_inputs)\n",
        "\n",
        "# if we try running this code, it will complain because the model inputs have not been padded\n",
        "#model (model_inputs)"
      ],
      "metadata": {
        "id": "eSfKEk4sxbmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Will pad the sequences up to the maximum sequence length\n",
        "\n",
        "## PADDING\n",
        "\n",
        "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
        "print (model_inputs)\n",
        "print (len(model_inputs['input_ids'][0]))\n",
        "print (len(model_inputs['attention_mask'][0]))"
      ],
      "metadata": {
        "id": "hZw8PTpux8j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Will pad the sequences up to the model max length\n",
        "# (512 for BERT or DistilBERT)\n",
        "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
        "print (model_inputs)\n",
        "print (len(model_inputs['input_ids'][0]))\n",
        "print (len(model_inputs['attention_mask'][0]))"
      ],
      "metadata": {
        "id": "spcFWQo6yZTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Will pad the sequences up to the specified max length\n",
        "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)\n",
        "print (model_inputs)\n",
        "print (len(model_inputs['input_ids'][0]))\n",
        "print (len(model_inputs['attention_mask'][0]))"
      ],
      "metadata": {
        "id": "SRvy_jxqycBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TRUNCATION\n",
        "# Will truncate the sequences that are longer than the model max length\n",
        "# (512 for BERT or DistilBERT)\n",
        "model_inputs = tokenizer(sequences, truncation=True)\n",
        "print (model_inputs)"
      ],
      "metadata": {
        "id": "vATIoIOhytqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Will truncate the sequences that are longer than the specified max length\n",
        "model_inputs = tokenizer(sequences, max_length=8, truncation=True)\n",
        "print (model_inputs)"
      ],
      "metadata": {
        "id": "FdoZbEAL0b6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## RETURN TENSORS\n",
        "\n",
        "# Will truncate the sequences that are longer than the specified max length\n",
        "model_inputs = tokenizer(sequences, max_length=8, truncation=True, padding = True, return_tensors = 'tf')\n",
        "print (model_inputs)"
      ],
      "metadata": {
        "id": "ycAvbRGC0i4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## RETURN TENSORS\n",
        "\n",
        "# Will truncate the sequences that are longer than the specified max length\n",
        "model_inputs = tokenizer(sequences, truncation=True, padding = 'longest', return_tensors = 'pt')\n",
        "print (model_inputs)"
      ],
      "metadata": {
        "id": "AbPKEnXq1cO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")\n",
        "print (model_inputs)"
      ],
      "metadata": {
        "id": "ghP_u6sw1wJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
        "\n",
        "model_inputs = tokenizer(sequence)\n",
        "print(model_inputs[\"input_ids\"])\n",
        "\n",
        "tokens = tokenizer.tokenize(sequence)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids) #there is an extra ID added at the starting and end position when we use tokenizer\n",
        "\n",
        "print (tokenizer.decode(model_inputs[\"input_ids\"]))\n",
        "\n",
        "print (print (tokenizer.decode(ids)))"
      ],
      "metadata": {
        "id": "YH3cMq4q2HtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Wrapping it up\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
        "\n",
        "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "output = model(**tokens)\n",
        "print (output)"
      ],
      "metadata": {
        "id": "zaWADe3R2umO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## QUIZ\n",
        "\n",
        "from transformers import AutoTokenizer, TFAutoModel # When using TF.. TF before AutoModel is critical!\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "seq = 'My time is limited, there is a lot of things to do'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "input_model = tokenizer (seq, return_tensors = 'tf')\n",
        "\n",
        "model = TFAutoModel.from_pretrained (checkpoint)\n",
        "output_model = model (input_model)\n",
        "\n",
        "print (output_model)"
      ],
      "metadata": {
        "id": "e-5vWy_L4Uzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SUGGESTION FROM DEEPSEEK\n",
        "\n",
        "# from transformers import AutoTokenizer, TFAutoModel  # Note: Using TFAutoModel for TensorFlow\n",
        "# import tensorflow as tf\n",
        "\n",
        "# checkpoint = \"bert-base-uncased\"  # Explicitly define the checkpoint\n",
        "# seq = 'My time is limited, there is a lot of things to do'\n",
        "\n",
        "# # Initialize tokenizer and model\n",
        "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "# model = TFAutoModel.from_pretrained(checkpoint)  # Using TensorFlow variant\n",
        "\n",
        "# # Tokenize input (returns TensorFlow tensors)\n",
        "# inputs = tokenizer(seq, return_tensors=\"tf\")\n",
        "\n",
        "# # Get model outputs\n",
        "# outputs = model(**inputs)\n",
        "\n",
        "# # Print outputs\n",
        "# print(\"Last hidden states shape:\", outputs.last_hidden_state.shape)\n",
        "# print(\"Pooler output shape:\", outputs.pooler_output.shape)"
      ],
      "metadata": {
        "id": "slzuyht29W2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WEEK 8\n",
        "Fine tuning a pre-trained model"
      ],
      "metadata": {
        "id": "NUs8lrMFJdY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# import numpy as np\n",
        "# from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "# # Same as before\n",
        "# checkpoint = \"bert-base-uncased\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "# model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "# sequences = [\n",
        "#     \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "#     \"This course is amazing!\",\n",
        "# ]\n",
        "\n",
        "# batch = dict(tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\"))\n",
        "\n",
        "# # This is new\n",
        "# model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
        "# labels = tf.convert_to_tensor([1, 1])\n",
        "# model.train_on_batch(batch, labels)"
      ],
      "metadata": {
        "id": "H1_yaNPMLg-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I changed line 14 and 18 of the previous code\n",
        "# This part sort of introduces fine-tuning a model from hugging face using own\n",
        "# data.. this data has just 2 sentenses so i presume it will be shit!\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "# Same as before\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "sequences = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"This course is amazing!\",\n",
        "]\n",
        "\n",
        "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "\n",
        "# This is new\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\")\n",
        "\n",
        "labels = tf.constant([1, 1])\n",
        "model.train_on_batch(batch, labels)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "rsieTArTLmDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on new text\n",
        "new_sequences = [\"I hate her\", \"Today was horrible.\"]\n",
        "new_batch = tokenizer(new_sequences, padding=True, truncation=True, return_tensors=\"tf\")\n",
        "predictions = model.predict(new_batch)\n",
        "print(predictions.logits)  # Raw output scores"
      ],
      "metadata": {
        "id": "E9gD-vEsNg6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probabilities = tf.math.softmax(predictions.logits, axis=-1)\n",
        "print(probabilities)  # Class probabilities"
      ],
      "metadata": {
        "id": "I4ER46u2N74l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.config)"
      ],
      "metadata": {
        "id": "dnppO0y8S-Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.id2label"
      ],
      "metadata": {
        "id": "uUDvEXzcTEPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "jr1FZr1EJneD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "df = pd.DataFrame (raw_datasets ['train']) # train to dataframe for better view"
      ],
      "metadata": {
        "id": "tTO5S4qbJfHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0,:]"
      ],
      "metadata": {
        "id": "GH6SPQj-Jj1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (df.iloc[0,0])\n",
        "print (df.iloc[0,1])\n",
        "print (df.iloc[1,0])\n",
        "print (df.iloc[1,1])"
      ],
      "metadata": {
        "id": "Ms1lcepyKpsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (type (raw_datasets)) #not a dictionary.. it is a dataset object\n",
        "print (raw_datasets)"
      ],
      "metadata": {
        "id": "ANH7neR2Z2rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I guess this is a better view\n",
        "print (raw_datasets['train'][0])\n",
        "print (raw_datasets['train'][1])"
      ],
      "metadata": {
        "id": "Ov57fuESV3sv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This is super weird for now\n",
        "raw_datasets['train'].features"
      ],
      "metadata": {
        "id": "8qpOi7afXQpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after the train access.. the dataset object sort of becomes like a dataframe, where the features are accessed by their names\n",
        "print (raw_datasets['train']['sentence1'][0:3])\n",
        "print (raw_datasets['train']['sentence2'][0:3])\n",
        "print (raw_datasets['train']['label'][0:3])"
      ],
      "metadata": {
        "id": "qq8DtvN-XyF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sentense-pair 15\n",
        "raw_datasets ['train'][15]"
      ],
      "metadata": {
        "id": "NaeNSUQ9bok6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets ['train']['sentence1']"
      ],
      "metadata": {
        "id": "Y_8r7VxwjaoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TOKENIZATION\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained (checkpoint)\n",
        "\n",
        "# This approach is not proper, since we need the sentences to be used as a pair, not separately\n",
        "sentence1_tokenizer = tokenizer (raw_datasets['train']['sentence1'], padding = True, truncation = True, return_tensors = 'tf')\n",
        "sentence1_tokenizer = tokenizer (raw_datasets['train']['sentence1'], padding = True, truncation = True, return_tensors = 'tf')"
      ],
      "metadata": {
        "id": "D6Fx0F3TdYTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#output\n",
        "print (sentence1_tokenizer)"
      ],
      "metadata": {
        "id": "Lbw7x_TkBpg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If I use 1 or more sequences in my tokenizer, the token_type_ids will be identical\n",
        "# If I use pairs, it will be different, this is what has been used in the notebook above\n",
        "\n",
        "output_tokenizer = tokenizer (['My name is Daudi', 'I love NLP'], padding = True)\n",
        "print (output_tokenizer)\n",
        "\n",
        "output_tokenizer_2 = tokenizer ('My name is Daudi', 'I love NLP')\n",
        "print (output_tokenizer_2)\n",
        "\n",
        "# I can use list of sequences in each pair\n",
        "# This is critical because it highlights how even when you have many s1, s2 pairs, the id_types will still match\n",
        "# It helps explain how we can use this for our raw_datasets which has 2 lists each having many sequences\n",
        "output_tokenizer_3 = tokenizer (['My name is Daudi', 'I love NLP'],['I am trying to understand something', 'It is great!'])\n",
        "print (output_tokenizer_3)\n",
        "\n",
        "#using output_tokenizer as input failed\n",
        "#i think the expectation is to have just one set of input ids and not 2 or more\n",
        "# it is mentioned that token_type_ids may not be available in other models, eg distil-bert, i have to confirm this though\n",
        "\n",
        "print (tokenizer.convert_ids_to_tokens (output_tokenizer_2['input_ids']))\n",
        "\n",
        "\n",
        "print (tokenizer.convert_ids_to_tokens (output_tokenizer_3['input_ids'][0]))\n"
      ],
      "metadata": {
        "id": "du_wvp9ukPPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets['train']['sentence1']\n",
        "type (raw_datasets['train']['sentence1'])\n",
        "len (raw_datasets['train']['sentence1'])"
      ],
      "metadata": {
        "id": "O9jOJgc9oxPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenize the sequences as pairs instead of two different ones\n",
        "\n",
        "## TOKENIZATION\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained (checkpoint)\n",
        "\n",
        "# This approach is not proper, since we need the sentences to be used as a pair, not separately\n",
        "sentence_tokenizer = tokenizer (raw_datasets['train']['sentence1'], raw_datasets['train']['sentence2'] , padding = True, truncation = True, return_tensors = 'tf')"
      ],
      "metadata": {
        "id": "WcuUa5MrvJxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokenizer"
      ],
      "metadata": {
        "id": "kdGd-urSVvRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokenizer['token_type_ids'][0]"
      ],
      "metadata": {
        "id": "cg83WoSpVzeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokenizer['token_type_ids'][1]"
      ],
      "metadata": {
        "id": "B2KEz3PUeeg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like the approach we were using is not efficient if we want to pre-process our entire dataset, i.e the train, test and validation set. The best alternative will be to use define a tokenizer function, then use `Datasets.map()` to tokenize the entire dataset.\n",
        "\n",
        "An additional thing is that padding is now disabled,so the sequences will have varying length of input ids and we will use batch in our tokenization coz it will make things faster for us. I will also not return tensors coz they were not using them in the course materials.\n",
        "\n",
        "Think of this mapping as adding columns of a dataframe by applying a particular function to the original data frame.. the result will include all of the columns (by the way I have never tried this, I just think it is possible)"
      ],
      "metadata": {
        "id": "6sFr1tSFipX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Our mapping function\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
      ],
      "metadata": {
        "id": "JGcefOXwek9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we have a dataset that is tokenized\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "0Iete_uJigCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (tokenized_datasets['train'][0])"
      ],
      "metadata": {
        "id": "Q88K6ZqLkDao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (tokenized_datasets['train']['input_ids'])\n",
        "\n",
        "# These were input ids of the entire train dataset\n",
        "print (len(tokenized_datasets['train']['input_ids']))"
      ],
      "metadata": {
        "id": "qeFBfeiDlqsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input ids of the first first pair\n",
        "print (tokenized_datasets['train']['input_ids'][0])\n",
        "print (len (tokenized_datasets['train']['input_ids'][0]))\n",
        "\n",
        "# input ids of the second pair\n",
        "print (tokenized_datasets['train']['input_ids'][1])\n",
        "print (len (tokenized_datasets['train']['input_ids'][1]))"
      ],
      "metadata": {
        "id": "be4Oj08gmOaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This block takes the first 8 sentence-pairs in the tokenized train dataset\n",
        "# It creates a dictionary containing just label and the outputs of the tokenizer\n",
        "# The above columns are the ones that will be needed in training\n",
        "# It creates a list of input ids and checks its length (using list comp)\n",
        "# The result is the length of each sentense-pair input ids\n",
        "# The input ids all have varying length, min is 32, max is 67\n",
        "\n",
        "samples = tokenized_datasets[\"train\"][:8]\n",
        "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
        "[len(x) for x in samples[\"input_ids\"]]"
      ],
      "metadata": {
        "id": "MmdYTigQlhUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at components of a complex code help me break it down and understand\n",
        "print (tokenized_datasets[\"train\"][0])\n",
        "print (samples)\n",
        "print (samples.keys())"
      ],
      "metadata": {
        "id": "fs6WhmKNwIzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "#tokenizer is an input that helps the collator to know details about padding\n",
        "# details are such as which padding to use & whether to pad from left or right\n",
        "# different tokenizers come with different such details\n",
        "# we also return the output of the Collation as a tensor\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer = tokenizer, return_tensors='tf')\n",
        "\n",
        "batch = data_collator (samples)"
      ],
      "metadata": {
        "id": "rSb7iY8Nzmut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type (batch) # looks like it is a dictionary\n",
        "batch.keys() # keys are the same\n",
        "\n",
        "batch['input_ids'].shape # 8 tokenized sentence pairs, with 67 input id values"
      ],
      "metadata": {
        "id": "wmAdeJAez9nB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# looks like our tokenizer by default was padding wrt the longest sequence.. 67\n",
        " {k: v.shape for k,v in batch.items()}"
      ],
      "metadata": {
        "id": "9N5A-4E_1lIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can collate the train, test and validation tokenized datasets independently\n",
        "# In each set, we batch the set, and padding is done\n",
        "# Batching is done according to the longest sequence\n",
        "\n",
        "tokenized_train = tokenized_datasets['train'][:]\n",
        "tokenized_train = {k:v for k,v in tokenized_train.items() if k not in (['sentence1', 'sentence2', 'idx'])}\n",
        "\n",
        "tokenized_test = tokenized_datasets['test'][:]\n",
        "tokenized_test = {k:v for k,v in tokenized_test.items() if k not in (['sentence1', 'sentence2', 'idx'])}\n",
        "\n",
        "tokenized_validation = tokenized_datasets['validation'][:]\n",
        "tokenized_validation = {k:v for k,v in tokenized_validation.items() if k not in (['sentence1', 'sentence2', 'idx'])}\n",
        "\n",
        "train_collator = data_collator (tokenized_train)\n",
        "test_collator = data_collator (tokenized_test)\n",
        "validation_collator = data_collator (tokenized_validation)\n",
        "\n",
        "train_batch = {k:v.shape for k,v in train_collator.items()}\n",
        "test_batch = {k:v.shape for k,v in test_collator.items()}\n",
        "validation_batch = {k:v.shape for k,v in validation_collator.items()}\n",
        "\n",
        "# validation batch has the shortest padding\n",
        "# test and train are almost the same\n",
        "print (train_batch)\n",
        "print (test_batch)\n",
        "print (validation_batch)"
      ],
      "metadata": {
        "id": "GHP_LhsP2uah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to avoid all of the trouble of the previous cell, we can use this approach\n",
        "# We leave out the test set because it will not be needed for training\n",
        "\n",
        "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
        "    label_cols=[\"labels\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=8,\n",
        ")\n",
        "\n",
        "tf_validation_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
        "    label_cols=[\"labels\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=8,\n",
        ")"
      ],
      "metadata": {
        "id": "wULUuLNZ6h33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Looks like this new format is not a dictionary..\n",
        "# We'll confirm of its shape and everything in the next session\n",
        "\n",
        "type (tf_train_dataset)\n",
        "tf_train_dataset"
      ],
      "metadata": {
        "id": "WjJ0UD0S7zx4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## WEEK 9: FINE-TUNING A MODEL"
      ],
      "metadata": {
        "id": "fY60UCHHt4C7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recap:\n",
        "In the previous lab, all I did was learn how to tokenize a training dataset that is downloaded from hugging face. The process involved tokenizing the dataset, but using the collator function for padding the train and validation sets separately. The notebooks show the process until that was achieved in the last cell.\n",
        "\n",
        "__I think the same thing could have been archieved without even using the collator function. I have coded it below, but won't run until when I have more time__"
      ],
      "metadata": {
        "id": "GSMDp6SVuBp5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Our mapping function\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True, padding = True, return_tensors = 'tf')\n",
        "\n",
        "training = raw_datasets['train'].map(tokenize_function)\n",
        "validation = raw_datasets['train'].map(tokenize_function)"
      ],
      "metadata": {
        "id": "uCBzxo12uAnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This has to be run everytime I want to use datasets from huggingface\n",
        "\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "fho79whRcOZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## What we have done so far in summarized in 1 cell\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "import numpy as np\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
        "\n",
        "tf_train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
        "    label_cols=[\"labels\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=8,\n",
        ")\n",
        "\n",
        "tf_validation_dataset = tokenized_datasets[\"validation\"].to_tf_dataset(\n",
        "    columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n",
        "    label_cols=[\"labels\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        "    batch_size=8,\n",
        ")"
      ],
      "metadata": {
        "id": "jAL-6h9-t9Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the bert model\n",
        "# We need to use it for classifying into 2 labels only\n",
        "# We retain the head for embedding, etc, and add a classification head\n",
        "# I am not very much familiar with the transformers architecture so mouth is shut\n",
        "# Initially we never had the num_labels parameter, I guess it's bcoz we are fine-tuning\n",
        "\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ],
      "metadata": {
        "id": "zauehMr9boXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This model can be improved on by a couple of tweaks\n",
        "\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "\n",
        "# the model will give out logits in its final layer\n",
        "# we specify this to the loss function\n",
        "# alternatively, we could have just used loss = 'Sparse...'\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    tf_train_dataset,\n",
        "    validation_data=tf_validation_dataset,\n",
        ")"
      ],
      "metadata": {
        "id": "ldKDZGB6eIyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# THIS IS THE INITIAL CODE THAT DEVELOPED ERRORS IN THE COMPILATION OF THE MODEL\n",
        "\n",
        "# We introducing a decaying learning rate\n",
        "# The number of steps is not very clear, I will have to come back later\n",
        "\n",
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "\n",
        "batch_size = 8\n",
        "num_epochs = 3\n",
        "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n",
        "# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n",
        "# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n",
        "num_train_steps = len(tf_train_dataset) * num_epochs\n",
        "lr_scheduler = PolynomialDecay(\n",
        "    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",
        ")\n",
        "\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "opt = Adam(learning_rate=lr_scheduler)"
      ],
      "metadata": {
        "id": "HDXn8aWpdzlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SUGGESTION BY CHATGPT DID NOT WORK\n",
        "\n",
        "# We introducing a decaying learning rate\n",
        "# The number of steps is not very clear, I will have to come back later\n",
        "\n",
        "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
        "\n",
        "batch_size = 8\n",
        "num_epochs = 3\n",
        "\n",
        "# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n",
        "# by the total number of epochs. Note that the tf_train_dataset here is a batched tf.data.Dataset,\n",
        "# not the original Hugging Face Dataset, so its len() is already num_samples // batch_size.\n",
        "num_train_steps = len(tf_train_dataset) * num_epochs\n",
        "lr_scheduler = PolynomialDecay(\n",
        "    initial_learning_rate=5e-5, end_learning_rate=0.0, decay_steps=num_train_steps\n",
        ")\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "opt = tf.keras.optimizers.legacy.Adam(learning_rate=lr_scheduler)"
      ],
      "metadata": {
        "id": "VOzvtwlJmJHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To retrain our model we initialize it again\n",
        "\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    tf_train_dataset,\n",
        "    validation_data=tf_validation_dataset, epochs = 3\n",
        ")"
      ],
      "metadata": {
        "id": "j81fcz6hdWNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Eyx00xVNjXmg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}